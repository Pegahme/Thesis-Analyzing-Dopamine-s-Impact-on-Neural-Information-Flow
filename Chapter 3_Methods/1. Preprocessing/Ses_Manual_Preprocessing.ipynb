{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc81a8c-d5c0-4df4-be89-6e95af22ea6b",
   "metadata": {},
   "source": [
    "## Import Required Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da081ae0-d8b4-475a-b582-3d83ab3bf787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 0: Import required libraries for EEG preprocessing and analysis\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "from pyprep.prep_pipeline import PrepPipeline\n",
    "import pyprep\n",
    "from mne.preprocessing import ICA\n",
    "from autoreject import AutoReject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528ee65-8533-4643-a3b0-9eb4ccd35b5e",
   "metadata": {},
   "source": [
    "## Import Custom Logging and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c8b83-012b-4b1a-9889-736964182343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Import custom logging and preprocessing helper functions\n",
    "\n",
    "# These functions are defined in preproc_funcs_log.py and provide\n",
    "# logging, artifact regression, ICA inspection, plotting, and\n",
    "# reproducible saving/loading for subject-level EEG preprocessing.\n",
    "\n",
    "from preproc_funcs_log import (\n",
    "    setup_logging,\n",
    "    log_dropped_channels,\n",
    "    log_crop_data,\n",
    "    regress_out_accelerometer_artifacts,\n",
    "    log_eeg_regression_metrics,\n",
    "    plot_var,\n",
    "    log_bad_channels,\n",
    "    log_ica_parameters,\n",
    "    ica_and_plot_sources,\n",
    "    log_excluded_ica_sources,\n",
    "    plot_var_epochs,\n",
    "    save_preprocessed_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3204b20-418e-4b74-ba35-9810761a176f",
   "metadata": {},
   "source": [
    "## Define Data Paths, Load Metadata, and Select Subject/Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182c780-cb15-4e1b-9e9b-572960a7d9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Define data paths, load subject metadata, and select subject/session for analysis\n",
    "\n",
    "# Define raw data and output paths\n",
    "raw_dir = r\"P:\\ds003509\"\n",
    "preprocessed_dir = r\"C:\\Users\\pegah\\mne-python\\Master's Thesis\"\n",
    "excel_file = r\"C:\\Users\\pegah\\mne-python\\Master's Thesis\\preproc_ds003509.xlsx\"\n",
    "\n",
    "# Load metadata containing subject group, session, and clinical info\n",
    "metadata = pd.read_excel(excel_file)\n",
    "\n",
    "# Prompt user for subject ID, ensure 3-digit format\n",
    "sub_id = input(\"Enter the subject ID (e.g., '002'): \").strip().zfill(3)\n",
    "subject_id = f\"sub-{sub_id}\"\n",
    "\n",
    "# Find subject info in metadata\n",
    "row = metadata[metadata[\"participant_id\"] == subject_id]\n",
    "if row.empty:\n",
    "    raise ValueError(f\"Subject {subject_id} not found in metadata.\")\n",
    "\n",
    "# Extract group and medication session info\n",
    "group = row[\"Group\"].values[0]\n",
    "sess1_med = row[\"sess1_Med\"].values[0]\n",
    "sess2_med = row[\"sess2_Med\"].values[0]\n",
    "print(f\"\\nSubject {subject_id} is in group: {group}\")\n",
    "\n",
    "# Decide which session to load (ON/OFF for PD, single session for controls)\n",
    "if group == \"PD\":\n",
    "    print(f\"Available sessions: 1 → {sess1_med}, 2 → {sess2_med}\")\n",
    "    session_choice = input(\"Which session do you want to load? (Enter 1 or 2): \").strip()\n",
    "    if session_choice == \"1\":\n",
    "        session = \"01\"\n",
    "    elif session_choice == \"2\":\n",
    "        session = \"02\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid session choice. Must be 1 or 2.\")\n",
    "else:\n",
    "    if sess2_med != \"no s2\":\n",
    "        print(f\"Warning: Control subject {subject_id} has unexpected sess2 entry: {sess2_med}\")\n",
    "    session = \"01\"\n",
    "\n",
    "# Define BIDS path for the selected subject/session\n",
    "task_name = \"SimonConflict\"\n",
    "bids_path = BIDSPath(\n",
    "    root=raw_dir,\n",
    "    subject=sub_id,\n",
    "    session=session,\n",
    "    task=task_name,\n",
    "    datatype=\"eeg\",\n",
    "    extension=\".set\"\n",
    ")\n",
    "\n",
    "### Load the raw EEG data for this subject and session\n",
    "\n",
    "# -- Normally, we DO NOT suppress warnings, because MNE warnings are useful for\n",
    "# -- identifying data quirks (e.g., channel type mismatches, missing locations, 'boundary' events).\n",
    "# -- However, for batch processing or for a cleaner log, you can uncomment the lines below\n",
    "# -- to suppress warnings you expect and have already handled.\n",
    "# import warnings\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "#     raw = read_raw_bids(bids_path=bids_path, verbose=False)\n",
    "#     raw.load_data()\n",
    "\n",
    "# -- For interactive analysis, keep warnings visible:\n",
    "raw = read_raw_bids(bids_path=bids_path, verbose=False)\n",
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af992a7b-3292-443c-b7ac-99785996f1c6",
   "metadata": {},
   "source": [
    "## Initial Data Standardization: Annotation Cleaning, Channel Setup, and Montage Application\n",
    "\n",
    "This block performs the fundamental standardization steps required for robust EEG preprocessing across all subjects and sessions:\n",
    "\n",
    "- **Annotation cleaning:** Removes non-informative or disruptive markers (e.g., \"boundary\", \"STATUS\") from the dataset, ensuring clean event structure for epoching and artifact rejection.\n",
    "- **Output folder and logging initialization:** Creates dedicated directories and subject/session log files, supporting full reproducibility and detailed audit trails for every preprocessing decision.\n",
    "- **Channel type assignment:** Explicitly sets correct channel types (EEG, EOG, accelerometer, etc.) for all data channels, accommodating session-to-session or subject-to-subject variation.\n",
    "- **Montage application:** Applies the standard 10-20 electrode layout to guarantee spatial consistency across datasets.\n",
    "- **Unreliable channel dropping:** Removes known inconsistent channels (FT9, FT10, TP9, TP10) across subjects.\n",
    "- **Reference channel addition:** Inserts a flat CPz channel (absent due to its use as an online reference) to harmonize the channel set for all further steps, allowing future interpolation and re-referencing.\n",
    "- **Montage re-application:** Ensures any newly added channels are correctly registered in the standard montage.\n",
    "\n",
    "These initial steps are crucial for enabling downstream analyses such as ICA, source localization, and effective connectivity estimation, while ensuring all results are robust, comparable, and fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b386a3a-a971-41d8-8140-415ab7f7c4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Remove unwanted annotations (\"boundary\", \"STATUS\") from raw EEG data\n",
    "\n",
    "# \"boundary\" and \"STATUS\" annotations can indicate recording pauses,\n",
    "# data discontinuities, or status messages that are not meaningful for analysis.\n",
    "# Removing them ensures clean event structure for later epoching.\n",
    "\n",
    "labels_to_remove = [\"boundary\", \"STATUS\"]\n",
    "descriptions = np.array(raw.annotations.description)\n",
    "mask = np.zeros_like(descriptions, dtype=bool)\n",
    "\n",
    "for label in labels_to_remove:\n",
    "    exact_match = descriptions == label\n",
    "    prefix_match = np.char.startswith(descriptions, label + \"/\")\n",
    "    mask |= exact_match | prefix_match\n",
    "\n",
    "indices_to_delete = np.where(mask)[0]\n",
    "descriptions_to_delete = descriptions[indices_to_delete]\n",
    "\n",
    "if len(indices_to_delete) > 0:\n",
    "    raw.annotations.delete(indices_to_delete)\n",
    "    print(f\"Removed {len(indices_to_delete)} annotations:\")\n",
    "    for idx, desc in zip(indices_to_delete, descriptions_to_delete):\n",
    "        print(f\"  - Index {idx}: '{desc}'\")\n",
    "else:\n",
    "    print(\"No unwanted annotations ('boundary', 'STATUS') or their variants found.\")\n",
    "\n",
    "# This step helps prevent spurious or misaligned events in downstream epoching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88480244-5029-4a2f-9afd-07dc0b018f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Create output folders and initialize logging for the current subject/session\n",
    "\n",
    "# Create a subject-specific folder for all preprocessing outputs\n",
    "subject_preproc_folder = os.path.join(preprocessed_dir, subject_id)\n",
    "os.makedirs(subject_preproc_folder, exist_ok=True)\n",
    "\n",
    "# If the subject is a PD patient, create an additional session-specific folder\n",
    "session_id = f\"ses-{session}\"\n",
    "\n",
    "if group == \"PD\":\n",
    "    session_preproc_folder = os.path.join(subject_preproc_folder, session_id)\n",
    "    os.makedirs(session_preproc_folder, exist_ok=True)\n",
    "    save_dir = session_preproc_folder\n",
    "    print(f\"Created preprocessing folder: {session_preproc_folder}\")\n",
    "else:\n",
    "    save_dir = subject_preproc_folder\n",
    "    print(f\"Created preprocessing folder: {subject_preproc_folder}\")\n",
    "\n",
    "# Initialize a log file to record all preprocessing actions for this subject/session\n",
    "log_path = setup_logging(save_dir, subject_id)\n",
    "print(f\"Log file created at: {log_path}\")\n",
    "\n",
    "# Logging ensures every manual or automated step is tracked for full reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afa52e0-9887-4510-9e9d-907da49c6fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Assign correct channel types and apply the standard 10-20 EEG montage\n",
    "\n",
    "# Explicitly set special channel types (EOG, accelerometry, etc.)\n",
    "channel_types = {\n",
    "    'VEOG': 'eog',    # Vertical electrooculogram for artifact detection\n",
    "    'X': 'misc',      # Accelerometer channels (miscellaneous)\n",
    "    'Y': 'misc',\n",
    "    'Z': 'misc'\n",
    "}\n",
    "\n",
    "# Identify which of these special channels exist in this recording\n",
    "existing_special = {ch: ch_type for ch, ch_type in channel_types.items() if ch in raw.ch_names}\n",
    "raw.set_channel_types(existing_special)\n",
    "\n",
    "# Set all remaining channels to 'eeg'\n",
    "remaining = [ch for ch in raw.ch_names if ch not in existing_special]\n",
    "raw.set_channel_types({ch: 'eeg' for ch in remaining})\n",
    "print(f\"Special channels set: {existing_special}\")\n",
    "\n",
    "# Apply the standard 10-20 montage for consistent spatial referencing\n",
    "montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "raw.set_montage(montage, on_missing=\"ignore\")\n",
    "print(\"Montage successfully applied.\")\n",
    "\n",
    "# This standardization is essential for later re-referencing, ICA, and source localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f401c-e6c5-4884-a34e-d4dec53a1c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6: Drop predefined peripheral channels ('FT9', 'FT10', 'TP9', 'TP10') if present, and log the result\n",
    "\n",
    "# Dropping these four channels ensures consistency across subjects.\n",
    "\n",
    "required_channels = {'FT9', 'FT10', 'TP9', 'TP10'}\n",
    "raw = log_dropped_channels(log_path, subject_id, raw, required_channels)\n",
    "\n",
    "# The dropped channels are recorded in the subject/session log for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e0a6a-8dce-4dfc-9924-97e66580749b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 7: Add a flat CPz channel (reference) back into the dataset\n",
    "\n",
    "\"\"\"\n",
    "During recording, CPz served as the online reference and was not recorded as an active channel.\n",
    "To enable consistent re-referencing and later interpolation, a flat (zero-valued) CPz channel is added.\n",
    "This ensures all subjects/sessions share the same sensor layout, facilitating group analysis and source localization.\n",
    "\"\"\"\n",
    "\n",
    "n_times = raw.n_times\n",
    "sfreq = raw.info['sfreq']\n",
    "cpz_data = np.zeros((1, n_times))  # Flat signal\n",
    "\n",
    "info_cpz = mne.create_info(['CPz'], sfreq, ch_types='eeg')\n",
    "raw_cpz = mne.io.RawArray(cpz_data, info_cpz)\n",
    "\n",
    "raw.add_channels([raw_cpz], force_update_info=True)\n",
    "print(\"Flat CPz channel added back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3454c05-9c6d-462c-b3e9-7df1d5baca11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 8: Re-apply standard 10-20 EEG montage after adding CPz\n",
    "\n",
    "# Re-applying the montage ensures that the newly added CPz channel is spatially registered,\n",
    "# and that all electrode positions are consistently defined for subsequent processing (e.g., interpolation, source localization).\n",
    "\n",
    "raw.set_montage('standard_1020')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe997b-474d-4047-9cea-98bf4c8d69c8",
   "metadata": {},
   "source": [
    "## Data Segmentation: Separating Resting-State and Task Data\n",
    "\n",
    "This section isolates the pre-task (resting-state) EEG segment from the continuous recording and saves it for potential separate analysis. The remainder of the raw data is then cropped to include only the task-relevant period (beginning ~20 seconds before the first stimulus, for consistency among all subjects and sessions). These steps ensure precise temporal alignment for subsequent preprocessing and analysis, and help manage memory use for large datasets.\n",
    "\n",
    "- **Step 9:** Extract the first stimulus onset time to serve as a reference point for data segmentation.\n",
    "- **Step 10:** Crop the data to create and save the resting-state segment; then crop the raw data to exclude the resting portion, retaining only the task-related data for further analysis. Log the cropping action for full reproducibility.\n",
    "- **Step 11:** Clean up memory by deleting the now-unused resting-state object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b6121-2745-42b7-b3d3-22279e20cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Extract first stimulus event as reference point for segmentation\n",
    "\n",
    "events, _ = mne.events_from_annotations(raw)\n",
    "first_stim_sample = events[0, 0]\n",
    "first_stim_time = raw.times[first_stim_sample]\n",
    "print(f\"First stimulus at: {first_stim_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb661b-56d6-42cb-98df-b5be8c4010f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Crop and save the resting-state segment, crop task data, and log the crop\n",
    "\n",
    "# Save resting-state EEG (before first stimulus minus 20 seconds)\n",
    "resting_end = max(0, first_stim_time - 20)\n",
    "raw_rest = raw.copy().crop(tmin=0, tmax=resting_end)\n",
    "\n",
    "rest_dir = os.path.join(save_dir, f\"resting_state\")\n",
    "os.makedirs(rest_dir, exist_ok=True)\n",
    "resting_path = os.path.join(rest_dir, f\"{subject_id}_{session_id}_resting_raw.fif\")\n",
    "raw_rest.save(resting_path, overwrite=True)\n",
    "print(f\"Resting-state segment saved to: {resting_path}\")\n",
    "\n",
    "# Now crop the raw data to remove the resting portion for task analysis\n",
    "tmin = first_stim_time - 20\n",
    "raw = raw.crop(tmin=tmin)\n",
    "\n",
    "# Log cropping info for reproducibility\n",
    "tmax = raw.times[-1]\n",
    "duration = tmax - tmin\n",
    "log_crop_data(log_path, subject_id, tmin, tmax, duration)\n",
    "# raw_rest.plot()  # (Optional) plot the resting-state segment for quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25920237-6ce9-4c0e-94b4-15c74f2dae6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 11: Clean memory by deleting the resting-state data object\n",
    "\n",
    "import gc\n",
    "del raw_rest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef4b47-1cea-4012-adb1-340b84ad8970",
   "metadata": {},
   "source": [
    "## Initial Signal Cleaning, Artifact Regression, and Quality Assessment\n",
    "\n",
    "This block applies essential signal processing and quantitative assessment steps to maximize EEG data quality prior to advanced analyses:\n",
    "\n",
    "- **Variance visualization:** Computes and saves summary plots of signal variance over time and across channels for the raw data, providing a baseline quality overview.\n",
    "- **Bandpass filtering:** Applies a 0.5–50 Hz FIR filter (Hamming window) to remove slow drifts and high-frequency noise from EEG, EOG, and accelerometer channels.\n",
    "- **Accelerometer-based artifact regression:** Removes movement artifacts by regressing out filtered accelerometer signals (4–6 Hz) from the EEG channels using ridge regression; logs quantitative regression metrics per channel.\n",
    "- **Spectral and artifact inspection:** Computes and saves the post-filtering EEG power spectrum, visualizes filtered accelerometer signals, and enables before/after comparison plots for manual artifact assessment.\n",
    "- **Summary metrics:** Computes and saves sum of squares across channels (detecting large-scale transients) and channel-wise variance bar plots for post-cleaning quality control.\n",
    "- **State updates and memory management:** Updates the working dataset to the artifact-reduced version and closes figures to optimize memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec90ac-32be-448d-b6e1-ee93025535c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 12_a: Plot variance per EEG channel (bar plot for QC)\n",
    "\n",
    "channels_variance_plot = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_task_variance_channels_plot.png\")\n",
    "eeg_picks = mne.pick_types(raw.info, eeg=True)\n",
    "eeg_data = raw.get_data(picks=eeg_picks)\n",
    "eeg_channel_names = np.array(raw.info['ch_names'])[eeg_picks]\n",
    "channel_variances = np.var(eeg_data, axis=1)\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "x_positions = np.arange(len(eeg_channel_names))\n",
    "ax.bar(x_positions, channel_variances, color='blue')\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(eeg_channel_names, fontsize=10, rotation=45, ha=\"right\")\n",
    "ax.set_title(\"Variance of EEG Channels\", fontsize=14)\n",
    "ax.set_ylabel(\"Variance\", fontsize=12)\n",
    "ax.set_xlabel(\"EEG Channels\", fontsize=12)\n",
    "fig.savefig(channels_variance_plot, dpi=300, bbox_inches=\"tight\")\n",
    "#plt.show()\n",
    "plt.close(fig)\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"Channel variance bar plot saved to: {channels_variance_plot}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81831366-a1a3-4dc2-9ba1-42d35515f929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 12_b: Visualize variance over time and across channels (pre-cleaning)\n",
    "\n",
    "# Raw\n",
    "var_plot_save_path = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_task_variance_plot.png\")\n",
    "raw_stats = plot_var(raw, var_plot_save_path, title=\"Variance — Raw EEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da73e1f-b0e4-458a-a4f7-4cbf6ed97caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 12_c: RAW PSD plot\n",
    "picks_eeg = mne.pick_types(raw.info, eeg=True, exclude=['CPz'])  # exclude flat/synthetic CPz\n",
    "psd_raw = raw.compute_psd(picks=picks_eeg, fmax=70)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "psd_raw.plot(axes=ax, show=False)  # don't pop an interactive window\n",
    "ax.set_title('Power Spectral Density — Raw EEG (CPz excluded)', fontweight='bold')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Power')\n",
    "psd_raw_plot = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_psd_plot.png\")\n",
    "fig.savefig(psd_raw_plot, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5e2dc-34ac-4ac5-b77a-d9f8caaa853d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 13_a: Apply bandpass filter (0.5–50 Hz) to EEG, EOG, and misc channels\n",
    "\n",
    "raw_filtered = raw.copy().filter(\n",
    "    l_freq=0.5,\n",
    "    h_freq=50.0,\n",
    "    picks=['eeg', 'eog', 'misc'],\n",
    "    filter_length='auto',\n",
    "    l_trans_bandwidth='auto',\n",
    "    h_trans_bandwidth='auto',\n",
    "    n_jobs=-1,\n",
    "    method='fir',\n",
    "    phase='zero',\n",
    "    fir_window='hamming',\n",
    "    fir_design='firwin',\n",
    "    pad='reflect_limited'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321a292-3118-4831-b043-dad74f6086f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 13_b: Re-plot filtered variance if needed (post-regression)\n",
    "\n",
    "# Filtered\n",
    "var_plot_filtered_path = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_filtered_task_variance_plot.png\")\n",
    "filt_stats = plot_var(raw_filtered, var_plot_filtered_path, title=\"Variance — Filtered EEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977a7ba-4315-483a-88c1-3bb184f733f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 13_c: Plot EEG Power Spectral Density (PSD) after filtering\n",
    "# --- FILTERED PSD ---\n",
    "picks_eeg_f = mne.pick_types(raw_filtered.info, eeg=True, exclude=['CPz'])\n",
    "psd_filt = raw_filtered.compute_psd(picks=picks_eeg_f, fmax=70)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "psd_filt.plot(axes=ax, show=False)\n",
    "ax.set_title('Power Spectral Density — Filtered EEG (CPz excluded)', fontweight='bold')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Power')\n",
    "psd_filtered_plot = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_filtered_psd_plot.png\")\n",
    "fig.savefig(psd_filtered_plot, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b57a0-4d29-4462-b70a-6e7b23fff23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Regress out movement artifacts using accelerometer channels (4–6 Hz)\n",
    "\n",
    "raw_reg, accel_data_filtered = regress_out_accelerometer_artifacts(raw_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b888a09-e387-4a6f-9e9e-7c0338eb3b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 15: Log regression evaluation metrics for each EEG channel\n",
    "\n",
    "log_eeg_regression_metrics(\n",
    "    raw_before=raw_filtered,\n",
    "    raw_reg=raw_reg,\n",
    "    subject_id=subject_id,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a4741-01df-4f02-93d5-b401b70b2f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Optional) Step 16: Plot EEG Power Spectral Density (PSD) after filtering and regression\n",
    "# --- FILTERED PSD ---\n",
    "picks_eeg_f = mne.pick_types(raw_reg.info, eeg=True, exclude=['CPz'])\n",
    "psd_reg = raw_reg.compute_psd(picks=picks_eeg_f, fmax=70)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "psd_reg.plot(axes=ax, show=False)\n",
    "ax.set_title('Power Spectral Density — Filtered EEG (CPz excluded)', fontweight='bold')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Power')\n",
    "psd_reg_plot = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_reg_psd_plot.png\")\n",
    "fig.savefig(psd_reg_plot, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb88cd-5f8f-4e00-a1a8-827d218ff68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Optional) Step 17: Plot filtered accelerometer activity (4–6 Hz) for artifact inspection\n",
    "\n",
    "sfreq = raw.info['sfreq']\n",
    "times = np.arange(accel_data_filtered.shape[1]) / sfreq\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(times, accel_data_filtered[0], label='X axis')\n",
    "plt.plot(times, accel_data_filtered[1], label='Y axis')\n",
    "plt.plot(times, accel_data_filtered[2], label='Z axis')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude (filtered)')\n",
    "plt.title('Filtered Accelerometer Activity (4–6 Hz)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cd63d-007a-4b79-9d74-75d55e9d4e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Optional/manual QC) Step 18: Compare EEG before vs. after artifact regression visually \n",
    "\n",
    "fig_before = raw.plot(start=50.0, picks='eeg')\n",
    "#fig_before.axes[0].set_title('Before regression')\n",
    "fig_after = raw_reg.plot(start=50.0, picks='eeg')\n",
    "#fig_after.axes[0].set_title('After regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d188c5e-8436-4829-a9bf-eef5033976da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mne.viz import set_browser_backend\n",
    "set_browser_backend('matplotlib')  # ensures raw.plot returns a Matplotlib Figure\n",
    "\n",
    "START = 50.0\n",
    "FIGSIZE = (18, 9)\n",
    "DPI = 150\n",
    "\n",
    "# --- BEFORE regression (bandpass-filtered data) ---\n",
    "before_path = os.path.join(save_dir, f\"{subject_id}_{session_id}_EEG_beforeReg_browser.png\")\n",
    "fig_before = raw_filtered.plot(start=START, picks='eeg', show=False, block=False)\n",
    "fig_before.axes[0].set_title('')            # remove title\n",
    "fig_before.set_size_inches(*FIGSIZE)\n",
    "fig_before.savefig(before_path, dpi=DPI, bbox_inches='tight')\n",
    "plt.close(fig_before)\n",
    "\n",
    "# --- AFTER regression ---\n",
    "after_path = os.path.join(save_dir, f\"{subject_id}_{session_id}_EEG_afterReg_browser.png\")\n",
    "fig_after = raw_reg.plot(start=START, picks='eeg', show=False, block=False)\n",
    "fig_after.axes[0].set_title('')             # remove title\n",
    "fig_after.set_size_inches(*FIGSIZE)\n",
    "fig_after.savefig(after_path, dpi=DPI, bbox_inches='tight')\n",
    "plt.close(fig_after)\n",
    "\n",
    "with open(log_path, \"a\") as f:\n",
    "    f.write(f\"Saved browser BEFORE-reg: {before_path}\\n\")\n",
    "    f.write(f\"Saved browser AFTER-reg : {after_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e65670-6c4c-48d5-acd8-0122edc098d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 19: Update working raw with cleaned (artifact-reduced) version\n",
    "\n",
    "raw = raw_reg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5f9bd-a4f9-4c65-9ec0-4aecfbebe091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 20: Compute and plot sum of squares across EEG channels over time\n",
    "\n",
    "raw_sum_squares_plot = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_task_sum_squares_plot.png\")\n",
    "data, times = raw.get_data(return_times=True)\n",
    "sum_squares = np.sum(data ** 2, axis=0)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(times, sum_squares, label=\"Sum of Squares\", color=\"b\", linewidth=1.5)\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Sum of Squares Across Channels (µV²)\")\n",
    "plt.title(f\"Sum of Squares of EEG Channels Over Time - {subject_id}-{session_id}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(raw_sum_squares_plot, dpi=300, bbox_inches=\"tight\")\n",
    "#plt.show()\n",
    "plt.close()\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"Sum of Squares plot saved to: {raw_sum_squares_plot}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed0503-9d1c-4f66-893e-9dde61950b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 21: Plot variance per EEG channel (bar plot for QC)\n",
    "\n",
    "channels_variance_plot = os.path.join(save_dir, f\"{subject_id}_{session_id}_filtered_task_variance_channels_plot.png\")\n",
    "eeg_picks = mne.pick_types(raw.info, eeg=True)\n",
    "eeg_data = raw.get_data(picks=eeg_picks)\n",
    "eeg_channel_names = np.array(raw.info['ch_names'])[eeg_picks]\n",
    "channel_variances = np.var(eeg_data, axis=1)\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "x_positions = np.arange(len(eeg_channel_names))\n",
    "ax.bar(x_positions, channel_variances, color='blue')\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(eeg_channel_names, fontsize=10, rotation=45, ha=\"right\")\n",
    "ax.set_title(\"Variance of EEG Channels Post Filtering\", fontsize=14)\n",
    "ax.set_ylabel(\"Variance\", fontsize=12)\n",
    "ax.set_xlabel(\"EEG Channels\", fontsize=12)\n",
    "fig.savefig(channels_variance_plot, dpi=300, bbox_inches=\"tight\")\n",
    "#plt.show()\n",
    "plt.close(fig)\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"Channel variance bar plot saved to: {channels_variance_plot}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168815f9-54cd-42bf-ae6a-39f4eece7b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 22: Re-plot filtered variance if needed (post-regression)\n",
    "\n",
    "var_plot_filtered_path = os.path.join(save_dir, f\"{subject_id}_{session_id}_raw_filtered_task_variance_plot.png\")\n",
    "plot_var(raw, var_plot_filtered_path)\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"Filtered variance plot saved to: {var_plot_filtered_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92119f6b-21a6-4585-9905-a6b293c61dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 23: Close all matplotlib figures and free up memory\n",
    "\n",
    "plt.close('all')\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0e1b5-d74f-4933-86fa-4f5e4a2f8882",
   "metadata": {},
   "source": [
    "## Robust Referencing and Ocular Artifact Channel Correction\n",
    "\n",
    "This block standardizes and prepares the data for advanced artifact rejection and downstream analyses:\n",
    "\n",
    "- **PREP pipeline referencing:** Applies the PREP pipeline to robustly re-reference EEG data and automatically interpolate noisy/bad channels. This harmonizes channel sets and improves data quality for all subjects and sessions.\n",
    "- **Bad channel tracking:** Logs the set of interpolated (bad) channels for full reproducibility and future reference.\n",
    "- **Bad list reset:** Clears the 'bads' list after PREP interpolation, preventing mislabeling of these channels in later steps.\n",
    "- **VEOG channel verification/restoration:** Ensures a valid vertical EOG (VEOG) channel is present and contains physiological data. If missing or flat, attempts restoration from the original raw recording.\n",
    "    - *Rationale:* ICA-based artifact correction (next steps) critically depends on a valid EOG channel to identify and remove blink/eye movement artifacts. This check guarantees that ICA will function optimally and consistently for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f595e4-6fa9-4bec-abfc-3931191f964a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 24: Copy the current working raw data object for PREP processing\n",
    "raw_prep = raw\n",
    "\n",
    "# Step 25: Patch the pyprep removeTrend function to avoid errors/crashes\n",
    "def no_op_remove_trend(*args, **kwargs):\n",
    "    return args[0]\n",
    "pyprep.reference.removeTrend = no_op_remove_trend\n",
    "\n",
    "# Step 26: Create a list of EEG channels (excluding other types)\n",
    "eeg_chs = [ch for ch in raw.ch_names if raw.get_channel_types(picks=ch)[0] == 'eeg']\n",
    "\n",
    "# Step 27: Define PREP parameters for referencing and bad channel detection/interpolation\n",
    "prep_params = {\n",
    "    \"ref_chs\": eeg_chs,\n",
    "    \"reref_chs\": eeg_chs,\n",
    "    \"line_freqs\": [],  # No notch filtering in this pipeline; adjust if needed\n",
    "    \"max_iterations\": 4,\n",
    "}\n",
    "\n",
    "# Step 28: Run the PREP pipeline, skipping internal filtering, because the data has been already bandpass-filtered\n",
    "prep = PrepPipeline(raw_prep, prep_params, montage=None)\n",
    "prep._run_filter = lambda: None\n",
    "prep.raw_eeg.set_montage(\"standard_1020\")\n",
    "prep.fit()\n",
    "\n",
    "# Step 29: Replace raw_prep with the PREP-processed data, and record interpolated channels\n",
    "raw_prep = prep.raw_eeg\n",
    "interpolated_channels = prep.interpolated_channels\n",
    "\n",
    "# Step 30: Log interpolated (bad) channels for traceability\n",
    "bad_str = ', '.join(interpolated_channels) if interpolated_channels else \"None\"\n",
    "print(f\"PREP bad channels: {bad_str}\")\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"PREP bad channels (interpolated): {bad_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53be4f-fe20-4306-9a8f-b907211bf64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 31: Ensure that the 'bads' list is empty after interpolation (to avoid confusion for downstream steps)\n",
    "\n",
    "raw_prep.info['bads'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3631cf-f361-4c2d-aa0d-4b0d7b383e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 32: Ensure a valid VEOG channel is present (needed for accurate ICA artifact removal)\n",
    "# ICA relies on EOG channels to identify and remove ocular artifacts, so we must verify that the VEOG channel\n",
    "# exists and contains real data. If it is missing or invalid, attempt to restore it from the original raw data.\n",
    "\n",
    "if 'VEOG' in raw_prep.ch_names:\n",
    "    veog_type = raw_prep.get_channel_types(picks='VEOG')[0]\n",
    "    veog_data = raw_prep.get_data(picks='VEOG')\n",
    "\n",
    "    if veog_type != 'eog' or np.allclose(veog_data, 0):\n",
    "        print(\"VEOG is present but seems invalid. Replacing it.\")\n",
    "        raw_prep.drop_channels(['VEOG'])\n",
    "\n",
    "        if 'VEOG' in raw.ch_names:\n",
    "            veog_raw = raw.copy().pick(picks='VEOG')\n",
    "            raw_prep.add_channels([veog_raw], force_update_info=True)\n",
    "            print(\"VEOG channel restored from original raw.\")\n",
    "            with open(log_path, 'a') as f:\n",
    "                f.write(\"VEOG was invalid and replaced from original raw.\\n\")\n",
    "        else:\n",
    "            print(\"Original raw does not contain VEOG.\")\n",
    "            with open(log_path, 'a') as f:\n",
    "                f.write(\"VEOG was invalid and could not be restored (missing in original).\\n\")\n",
    "    else:\n",
    "        print(\"VEOG is already present and valid.\")\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(\"VEOG channel is valid post-PREP.\\n\")\n",
    "else:\n",
    "    if 'VEOG' in raw.ch_names:\n",
    "        veog_raw = raw.copy().pick(picks='VEOG')\n",
    "        raw_prep.add_channels([veog_raw], force_update_info=True)\n",
    "        print(\"VEOG channel re-attached.\")\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(\"VEOG was missing and re-attached from original raw.\\n\")\n",
    "    else:\n",
    "        print(\"VEOG not found in original raw.\")\n",
    "        with open(log_path, 'a') as f:\n",
    "            f.write(\"VEOG was missing and could not be added.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b05ca-de43-47a3-b51c-06a15b7ce500",
   "metadata": {},
   "source": [
    "## Artifact Correction via Independent Component Analysis (ICA)\n",
    "\n",
    "This block implements independent component analysis (ICA) for the identification and removal of non-neural artifacts—particularly those arising from ocular, muscle, and movement sources. ICA is a critical preprocessing step for EEG source analysis, as it enables the isolation and attenuation of statistically independent noise components, thereby maximizing the physiological relevance and signal-to-noise ratio of the subsequent source-localized data.\n",
    "\n",
    "- **Parameter logging:** All ICA decomposition parameters (number of components, frequency filtering, random seed, etc.) are logged for transparency and reproducibility.\n",
    "- **Component inspection:** Both interactive and static visualizations of the ICA components are generated, allowing for comprehensive manual evaluation of independent sources. Topographical maps of all components are saved for each subject/session.\n",
    "- **Component selection:** Artifact-related components (e.g., eye blinks, muscle bursts) are identified based on their spatial, temporal, and spectral characteristics. The indices of components marked for exclusion are explicitly recorded in the session log.\n",
    "- **Artifact removal:** The selected components are excluded from the data via back-projection, yielding a cleaned dataset optimized for subsequent source localization and connectivity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d063254-fed7-4189-9600-512ce75a5ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 33: Define and log ICA decomposition parameters\n",
    "\n",
    "n_components = 0.99  # Retain 99% variance\n",
    "l_freq = 1           # Lower cutoff for ICA input\n",
    "h_freq = None        # No upper cutoff for ICA input\n",
    "random_state = 42    # For reproducibility\n",
    "\n",
    "log_ica_parameters(log_path, subject_id, n_components, l_freq, h_freq, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37331879-a212-4316-99b8-68a425be8133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 34: Run ICA decomposition and generate component visualizations\n",
    "\n",
    "ica, ica_raw = ica_and_plot_sources(\n",
    "    raw_prep=raw_prep,\n",
    "    save_dir=save_dir,\n",
    "    subject_id=subject_id,\n",
    "    session_id=session_id,\n",
    "    n_components=n_components,\n",
    "    l_freq=l_freq,\n",
    "    h_freq=h_freq,\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf73be-0bd1-4ac3-8556-0c4ca68a7e76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Thesis figure: IC time series in a fixed window [25, 40] s\n",
    "# - ICA000 — Blink (VEOG‑correlated)\n",
    "# - ICA001 — Eye movement\n",
    "# - ICA004 — Heartbeat\n",
    "# Saves: {save_dir}/{subject_id}_{session_id}_IC_examples_timeseries_25to40s.png\n",
    "# ===========================================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from scipy.signal import find_peaks\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "\n",
    "# -------- window & ICs --------\n",
    "T_START = 25.0    # seconds\n",
    "T_END   = 40.0    # seconds\n",
    "ICS     = [0, 1, 4]  # [blink, eye movement, heartbeat]\n",
    "\n",
    "# -------- rebuild ICA input exactly as during fitting --------\n",
    "ica_input = raw_prep.copy().filter(l_freq=l_freq, h_freq=h_freq, verbose=False)\n",
    "times = ica_input.times\n",
    "sfreq = ica_input.info[\"sfreq\"]\n",
    "acts  = ica.get_sources(ica_input).get_data()  # (n_components, n_times)\n",
    "\n",
    "# clamp time window to available range\n",
    "t0 = max(T_START, float(times[0]))\n",
    "t1 = min(T_END,   float(times[-1]))\n",
    "i0 = int(np.searchsorted(times, t0))\n",
    "i1 = int(np.searchsorted(times, t1))\n",
    "if i1 - i0 < int(2 * sfreq):  # ensure at least ~2 s\n",
    "    i1 = min(len(times), i0 + int(2 * sfreq))\n",
    "\n",
    "# optional VEOG overlay for blink panel\n",
    "veog = None\n",
    "if \"VEOG\" in ica_input.ch_names:\n",
    "    veog = ica_input.copy().pick_channels([\"VEOG\"]).get_data()[0]\n",
    "\n",
    "# helper: mark within-window high-z events to guide the eye\n",
    "def shade_events(ax, t, y, sfreq, zthr=2.5, min_gap=0.30):\n",
    "    z = (y - y.mean()) / (y.std() + 1e-12)\n",
    "    if _HAS_SCIPY:\n",
    "        peaks, _ = find_peaks(np.abs(z), distance=int(min_gap * sfreq), height=zthr)\n",
    "        xs = t[peaks]\n",
    "    else:\n",
    "        xs = t[np.where(np.abs(z) > zthr)[0]]\n",
    "    for tt in xs:\n",
    "        ax.axvspan(max(t[0], tt - 0.05), min(t[-1], tt + 0.05), alpha=0.18)\n",
    "\n",
    "# -------- plot --------\n",
    "plt.rcParams.update({\"font.size\": 12})\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 6.2), constrained_layout=True)\n",
    "\n",
    "titles = [\n",
    "    \"ICA000 — Blink (VEOG‑correlated)\",\n",
    "    \"ICA001 — Eye movement\",\n",
    "    \"ICA004 — Heartbeat\",\n",
    "]\n",
    "\n",
    "for ax, ic, title in zip(axes, ICS, titles):\n",
    "    t = times[i0:i1]\n",
    "    y = acts[ic, i0:i1]\n",
    "    ax.plot(t, y, linewidth=1.1)\n",
    "    ax.set_ylabel(f\"ICA{ic:03d}\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    shade_events(ax, t, y, sfreq)\n",
    "\n",
    "    # VEOG overlay only for blink panel\n",
    "    if ic == 0 and veog is not None:\n",
    "        y2 = veog[i0:i1]\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(t, y2, alpha=0.75, linewidth=0.9)\n",
    "        ax2.set_ylabel(\"VEOG\", rotation=270, labelpad=14)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "fig.suptitle(f\"IC activations (fixed window {t0:.0f}–{t1:.0f} s) — sub-{subject_id}, {session_id}\", fontsize=14)\n",
    "\n",
    "out_path = Path(save_dir) / f\"{subject_id}_{session_id}_IC_removed_timeseries_{int(t0)}to{int(t1)}s.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1fae73-99cb-4b48-b52a-627e5b1e3438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 35: Save static ICA topomap (all components) for documentation and quality control\n",
    "\n",
    "ica_components = os.path.join(save_dir, f'{subject_id}_{session_id}_ICA_components_plot.png')\n",
    "fig = ica.plot_components(show=False, nrows=7, ncols=6)\n",
    "fig.savefig(ica_components, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"ICA components topomap saved to: {ica_components}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb0305-7c57-4e94-ace1-fa97be92207d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 36: Manually define components for exclusion based on visualization\n",
    "\n",
    "ica_exclude = [0, 1, 4]  # Update it based on component inspection\n",
    "\n",
    "log_excluded_ica_sources(log_path, subject_id, ica_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657384e-1c05-441e-bbad-89103bb80cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 37: Apply ICA to remove marked components and update the working dataset\n",
    "\n",
    "ICA_final = ica.apply(ica_raw, exclude=ica_exclude)\n",
    "# Optional: ICA_final.plot()  # Visualize cleaned data if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b053a-6592-4b3a-8b36-c7d8f57766fa",
   "metadata": {},
   "source": [
    "## Event Extraction and Epoching: Focusing on Training Phase Stimuli\n",
    "\n",
    "This block extracts stimulus onset events corresponding to the training phase (\"Trn Stim\") from the ICA-cleaned data and epochs the EEG accordingly. This segmentation is crucial for isolating peri-stimulus neural dynamics and aligning the dataset for downstream analyses (e.g., source localization, effective connectivity).\n",
    "\n",
    "- **Step 38:** Set the working dataset to the ICA-cleaned data, ensuring all further analyses are performed on artifact-corrected signals.\n",
    "- **Step 39:** Parse all annotated events and select only those corresponding to the training phase stimuli. This ensures the analysis targets the cognitive control mechanisms of interest, rather than unrelated task segments.\n",
    "- **Step 40:** Create epochs spanning -1.5 s to 5.0 s relative to each selected training stimulus onset. This wide window captures both pre-stimulus baseline activity and delayed responses, providing maximal flexibility for subsequent temporal analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23368f2b-eb5e-41c5-a8ac-dc3243fd39df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 38: Assign ICA-cleaned data as the input for epoching\n",
    "\n",
    "ICA_to_epoch = ICA_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170ed68-01c0-4bb3-8168-d00bb22a2a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 39: Extract stimulus events from cleaned data and filter for training phase\n",
    "\n",
    "events, event_id = mne.events_from_annotations(ICA_to_epoch)\n",
    "print(\"Available event labels:\", list(event_id.keys()))\n",
    "\n",
    "# Filter event IDs to select only 'Trn Stim' (training phase) stimuli\n",
    "stim_event_id = {k: v for k, v in event_id.items() if \"Trn Stim\" in k}\n",
    "print(f\"Selected stimulus event IDs ({len(stim_event_id)}):\", stim_event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058570c-1949-42b9-ab18-2dd26fdf6fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 40: Epoch the data around each training stimulus event\n",
    "# Epochs span from -1.5 s (pre-stimulus) to 5.0 s (post-stimulus), without baseline correction\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    ICA_to_epoch,\n",
    "    events=events,\n",
    "    event_id=stim_event_id,\n",
    "    tmin=-1.5,\n",
    "    tmax=5.0,\n",
    "    baseline=None,\n",
    "    picks=\"eeg\",\n",
    "    preload=True\n",
    ")\n",
    "print(f\"Number of epochs: {len(epochs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867834a-13c9-48fb-8ee2-447049c777bf",
   "metadata": {},
   "source": [
    "## ERP Visualization and Automated Epoch Cleaning\n",
    "\n",
    "This section provides both optional and essential quality control for the preprocessed epochs:\n",
    "\n",
    "- **Visual quality check (optional):**  \n",
    "  The grand-average event-related potential (ERP) is computed across all selected epochs, and a joint plot is generated to show both the temporal dynamics (ERP time course) and the spatial distribution (topomap) of the evoked response. This visualization serves as an important sanity check, allowing the experimenter to visually confirm the presence and expected morphology of task-related brain responses before proceeding with automated artifact handling. While not required for pipeline reproducibility, this step is highly recommended for comprehensive data quality assurance and documentation.\n",
    "\n",
    "- **Automated artifact rejection/interpolation:**  \n",
    "  To objectively remove artifact-contaminated data, the AutoReject algorithm is applied to all epochs. AutoReject identifies outlier channels and epochs, and uses a data-driven consensus procedure to either interpolate or exclude bad trials, thus maximizing data retention without compromising quality. The algorithm’s actions—including the number of retained/rejected epochs, which epochs were dropped, and the interpolation thresholds—are logged in detail for transparency and reproducibility.\n",
    "\n",
    "- **Quality control plots and logs:**  \n",
    "  Visual summaries of the artifact rejection process are generated and saved, allowing for transparent reporting and later review. All relevant file paths and summary statistics are written to the processing log to maintain a complete audit trail.\n",
    "\n",
    "By combining optional visual inspection with systematic, automated artifact rejection, this section ensures that only high-quality, artifact-free epochs are carried forward to advanced analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9969dc-ff52-4eb9-9ab8-46b3515f9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Step 41: Average all epochs and plot joint ERP/topomap for data quality assessment\n",
    "\n",
    "epochs_avg_joint = epochs.average()\n",
    "fig = epochs_avg_joint.plot_joint(title='Average ERP after Bandpass Filter')\n",
    "\n",
    "# Save plot\n",
    "epochs_avg_joint_plot = os.path.join(save_dir, f'{subject_id}_{session_id}_epochs_avg_joint.png')\n",
    "fig.savefig(epochs_avg_joint_plot, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Joint ERP plot saved at: {epochs_avg_joint_plot}\")\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"Joint ERP plot saved: {epochs_avg_joint_plot}\\n\")\n",
    "\n",
    "# Optional: Plot all epochs for manual inspection if needed\n",
    "# epochs.plot()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d0d6e-7a8c-4cd0-a387-de7c2e01c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 42: Apply AutoReject for automatic artifact rejection/interpolation across all epochs\n",
    "\n",
    "ar = AutoReject(\n",
    "    n_interpolate=[4],\n",
    "    consensus=[0.75],\n",
    "    n_jobs=4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ar.fit(epochs)  # Verbose output shown in notebook\n",
    "epochs_ar, reject_log = ar.transform(epochs, return_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf6d7b-2054-4e2e-9d3f-9a1b4c236c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 43: Log summary statistics after AutoReject (epochs rejected/interpolated)\n",
    "\n",
    "n_epochs_total = len(epochs)\n",
    "n_epochs_rejected = reject_log.bad_epochs.sum()\n",
    "n_epochs_retained = n_epochs_total - n_epochs_rejected\n",
    "dropped_epochs = np.where(reject_log.bad_epochs)[0]\n",
    "\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(\"\\n=== AutoReject Summary ===\\n\")\n",
    "    f.write(f\"Total epochs: {n_epochs_total}\\n\")\n",
    "    f.write(f\"Rejected epochs: {n_epochs_rejected}\\n\")\n",
    "    f.write(f\"Retained epochs: {n_epochs_retained}\\n\")\n",
    "    f.write(f\"Dropped epoch indices: {', '.join(map(str, dropped_epochs))}\\n\")\n",
    "    f.write(f\"Interpolation levels used: {ar.n_interpolate}\\n\")\n",
    "    f.write(f\"Consensus thresholds: {ar.consensus}\\n\")\n",
    "    f.write(\"AutoReject applied successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120d024-59fa-4b60-a623-5b8ced8976db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 44: Plot and save the AutoReject log for QC/visual reporting\n",
    "\n",
    "fig = reject_log.plot('horizontal')\n",
    "reject_log_plot_path = os.path.join(save_dir, f\"{subject_id}_{session_id}_autoreject_log.png\")\n",
    "fig.savefig(reject_log_plot_path, dpi=300, bbox_inches='tight')\n",
    "# plt.close(fig)\n",
    "\n",
    "print(f\"Autoreject log plot saved at: {reject_log_plot_path}\")\n",
    "\n",
    "# Log the file path\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f\"Autoreject log plot saved: {reject_log_plot_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6326f8-c011-4766-8c10-5461ca139edc",
   "metadata": {},
   "source": [
    "## Final Quality Control, Visualization, and Data Export\n",
    "\n",
    "- **Variance Visualization:** The variance of the cleaned, artifact-corrected epochs is visualized both across time and across channels, providing a final quality check and ensuring no unexpected artifacts or variance imbalances remain after all cleaning steps.\n",
    "- **Manual Epoch Inspection:** The full set of cleaned epochs is visualized interactively. This allows for a last, manual inspection to confirm data quality, detect any rare, persistent artifacts, and validate the results of automated cleaning.\n",
    "- **Export and Documentation:** The final preprocessed, cleaned epochs are saved to disk in MNE’s FIF format, along with a backup of the preprocessing notebook for full reproducibility. All key actions and file paths are logged to the session’s log file, ensuring traceability and transparency for every subject and session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4102d-aec1-448b-a692-83dd766df49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 45: Visualize variance and quality of cleaned epochs\n",
    "\n",
    "plot_var_epochs(\n",
    "    epochs_ar,\n",
    "    save_dir=save_dir,\n",
    "    subject_id=subject_id,\n",
    "    session_id=session_id,\n",
    "    log_path=log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f525a-6950-4980-9953-d9a9332c19a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 46: Visual inspection of final preprocessed epochs\n",
    "\n",
    "preproc_data = epochs_ar\n",
    "preproc_data.plot()  # Inspect final preprocessed epochs for unexpected artifacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaab84-a79e-442a-abd9-163f4feed9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 47: Save final preprocessed epochs and back up pipeline notebook\n",
    "\n",
    "fif_path, notebook_backup = save_preprocessed_data(\n",
    "    preproc_data=preproc_data,\n",
    "    save_dir=save_dir,\n",
    "    subject_id=subject_id,\n",
    "    session_id=session_id,\n",
    "    notebook_name=f\"{subject_id}_{session_id}_task_preproc.ipynb\",\n",
    "    log_path=log_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
